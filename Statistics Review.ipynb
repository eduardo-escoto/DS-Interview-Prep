{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Review for Interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Axioms\n",
    "Introduced by Kolmogorov to formalize Probability Spaces. Cox's Theorem provides alternative formulations of probability that tend to be preferred by Bayesians.\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a measure space. Where $P$ is the probability measure, $\\Omega$ is the sample space, $\\mathcal{F}$ is the event space. Then we have the following axioms:\n",
    "\n",
    "1. $P(E) \\in \\mathbb{R}, P(E) \\geq 0 \\quad \\forall E \\in \\mathcal{F}$\n",
    "    - Here we see that the probability is a non-negative real number for any set $E$ defined in the event space $\\mathcal{F}$.\n",
    "    - It follows that the probability measure is always finite from the next axiom in combination with this one.\n",
    "2. $P(\\Omega) = 1$\n",
    "    - This is the assumption of the unit measure. Intuitively, this axiom states that the probability of the sample space, $\\Omega$ is $1$.\n",
    "3. $P\\left(\\bigcup^{\\infty}_{i = 1} E_i\\right) = \\sum_{i = 1}^{\\infty} P(E_i), \\quad E_{k} \\cap E_{j} = \\emptyset, j \\neq k$\n",
    "    - This is the assumption of $\\sigma$-additivity in measures. \n",
    "    - In other words, the probability measure of the unions of disjoint events is the sum of probability measures of each event.\n",
    "    \n",
    "From these three axioms, the rest of probability can be derived as a consequence of the axioms and other properties from measure theory.\n",
    "\n",
    "**Note: Qausiprobabilities relax the third axiom of $\\sigma$-additivity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Consequences of the Probability Axioms\n",
    "\n",
    "#### Monotonicity\n",
    "Intuitively, an event $A$ is a subset of an event $B$, then the probability measure of $A$ should be less than $B$. So more formally we have:\n",
    "$$A \\subseteq B \\implies P(A) \\leq P(B)$$\n",
    "\n",
    "**Proof Idea: Show that $B$ can be composed of $A$ and some other sets from $B$ by using set excision.**\n",
    "\n",
    "#### Empty Set\n",
    "A null event should have probability of $0$. \"No event happening\" shouldn't be assigned a probability logically. Formally:\n",
    "$$P(\\emptyset) = 0$$\n",
    "Important to note, that for some event $E$, that $P(E) = 0 \\nRightarrow E = \\emptyset$. In other words, an event with probability zero does not mean that the event is the null set. To put it another way, the empty set is not the only set with probability measure $0$.\n",
    "\n",
    "**Proof Idea: Use a similar idea from the monotonicity prove, except prove by contradiction, where if $P(\\emptyset) \\neq 0$ then the sum of the measure of the sets goes to infinity, so its necessarily $0$.**\n",
    "\n",
    "#### The Complement\n",
    "The complement of an event, will yield $1$ minus the probability of that event. This can easily be seen since $A \\cup A^{c} = \\Omega  \\quad \\forall A \\in \\Omega$. So formally:\n",
    "$$\n",
    "P(A^{c}) = P(\\Omega \\setminus A) = 1 - P(A)\n",
    "$$\n",
    "\n",
    "**Proof Idea: Just use the Kolmogorov Axioms, its easy :p**\n",
    "\n",
    "#### Probability Measure Bounds\n",
    "From monoticity, and the measure of the full sample space, we can see that the probability should be bounded below by $0$ and above by $1$, i.e:\n",
    "$$\n",
    "0 \\leq P(E) \\leq 1 \\quad \\forall E \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "**Proof Idea: Use complement rule and the axioms, this one is also easy :D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Probability Properties\n",
    "\n",
    "#### Addition Rule\n",
    "For any two sets $A,B \\in \\Omega$, we have $P(A \\cup B) = P(A) + P(B) - P(A\\cap B)$. This is because in this case, we don't know if $A$ and $B$ are disjoint, so we must subtract the intersection. This idea is generalized by the Principle of Inclusion Exclusion from combinatorics for greater numbers of sets. \n",
    "\n",
    "#### Principle of Inclusion Exclusion\n",
    "For general $A_1, \\dots, A_n$, we have the following:\n",
    "$$\n",
    "P\\left(\\bigcup_{i = 1}^{n} A_{i}\\right) = \\sum_{i = 1}^{n}P(A_i) - \\sum_{i < j}P(A_i \\cap A_j) + \\sum_{i < j < k}P(A_i \\cap A_j \\cap A_k) + \\cdots + (-1)^{n-1}P\\left(\\bigcap_{i=1}^{n} A_{i}\\right)\n",
    "$$\n",
    "Which can be easily remembered. When the elements are disjoint however, they can be summed without inclusion exclusion, as the pairwise intersections will be the empty set, therefore causing all of the intersection terms to be 0.\n",
    "\n",
    "#### Some useful ones to remember\n",
    "- $P(A\\cap B) \\geq P(A) + P(B) - 1$\n",
    "- $P(A\\cap B) = P(A) + P(B) - P(A \\cup B)$\n",
    "- $B\\perp A \\implies P(A\\cap B) = P(A)*P(B)$ i.e when (A, B independent)\n",
    "- $P(A \\cap A^{c}) = 0$ since $A \\cap A^{c} = \\emptyset$\n",
    "- $P(A \\cup A^{c}) = 1$ since $A \\cup A^{c} = \\Omega$\n",
    "\n",
    "#### Commutativity, Associativity, De Morgan's, etc\n",
    "An important feature to note, is that all of the set theoretic operations can be translated to probabilities from the previous properties. For example, the De Morgan's law:\n",
    "$$ (A \\cup B)^{c} = A^{c} \\cap B^{c} \\implies P\\left((A \\cup B)^{c}\\right) = P\\left(A^{c} \\cap B^{c}\\right)$$\n",
    "which can then be simplified using properties we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "The conditional probability of $A$ given $B$ is defined as:\n",
    "$$\n",
    "P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "so then this yields a definition for independence. \n",
    "We have that $P(A)P(B) = P(A \\cap B)$ if and only if $A \\perp B$. \n",
    "\n",
    "In other words, when $A\\perp B$ we have that $P(A \\vert B) = P(A)$.\n",
    "\n",
    "This definition leads to Law of total probabilty and Bayes' Theorem.\n",
    "\n",
    "#### Law of total Probability\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^{n}P(A\\vert B_i)P(B_i)\n",
    "$$\n",
    "\n",
    "#### Bayes' Theorem\n",
    "$$\n",
    "P(B\\vert A) =\\frac{P(A\\cap B)}{P(A)} = \\frac{P(A\\vert B)P(B)}{P(A)} = \\frac{P(A\\vert B)P(B)}{P(A\\vert B)P(B) + P(A\\vert B^{c})P(B^{c})}\n",
    "$$\n",
    "Which we got from using the definition the intersection on the numerator. Furthermore, this can be extended by using the law of total probability for the denominator. For any mutually disjoint, and exhaustive $A_i$, we then have:\n",
    "$$\n",
    "P(B_i\\vert A) = \\frac{P(A\\vert B_i)P(B_i)}{\\sum_{i=1}^{n}P(A\\vert B_i)P(B_i)}\n",
    "$$\n",
    "\n",
    "An easy Bayes' Theorem question is the classic drug testing question. [See here for example and solution](https://en.wikipedia.org/wiki/Bayes%27_theorem#Drug_testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionally Independent Random Variables\n",
    "- $Y_1, \\dots Y_n$ are random variables. \n",
    "- We say they are conditionally independent given $\\theta$ if each $Y_i$ only gives information about $\\theta$ and not about any other $Y_j$. \n",
    "\n",
    "Formally, given events $A,B,C$, then $A$ and $B$ are conditionally independent given $C$ if $P(A \\cap B\\vert C) = P(A\\vert C)P(B\\vert C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood:\n",
    "- The \"probability of the observed data\" expressed as a function of the parameter\n",
    "- Likelihood is NOT a distribution\n",
    "- Depends on the observed data\n",
    "- Asymptotically approaches the true value of the parameter\n",
    "- Unbiased estimator, may be less performant with smaller samples vs a biased estimator\n",
    "\n",
    "If the samples are conditionally independent, i.e. i.i.d or independent, then we can express probabilities as:\n",
    "$$P(y_1,\\dots, y_n \\vert \\theta) = P(y_1 \\cap \\dots \\cap y_n \\vert \\theta) = P(y_1 \\vert \\theta)\\cdot\\dots\\cdot P(y_n \\vert \\theta)$$\n",
    "\n",
    "Which then leads to the definition as:\n",
    "$$L(\\theta \\vert y_1, \\dots, y_n) = P(y_1, \\dots, y_n \\vert \\theta) = \\prod_{k=1}^{n} P(y_k \\vert \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "- We essentially estimate the parameter value $\\theta$ by maximizing the likelihood function $L(\\theta)$.\n",
    "- Often times we use calculus. I.E take partial derivatives and solve for where $L'(\\theta) = 0$\n",
    "- Often times more useful to use the log-likelihood, $\\ell(\\theta) = \\log \\left\\{L(\\theta)\\right\\}$ as this turns products into sums and makes solving easier.\n",
    "    - Note that the log is monotonic, and takes the maximum at the same value of $\\theta$ as the likelihood.\n",
    "- Sometimes cannot be solved for analytically, so we can solve numerically in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value, Mean, and Sample Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and Sample Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation, and Sample Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probability Distributions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
